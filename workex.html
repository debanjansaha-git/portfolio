<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>My Work Experiences</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html#nav" class="logo">Portfolio</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li class="active"><a href="workex.html">WorkEx</a></li>
							<li><a href="projects.html">Projects</a></li>
							<li><a href="aboutme.html">About Me</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/debanjanaiml/" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://github.com/debanjansaha-git" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="http://www.youtube.com/@user-wf8dm8wu9b" class="icon brands alt fa-youtube"><span class="label">YouTube</span></a></li>
							<li><a href="https://medium.com/@dsdojo" class="icon brands alt fa-medium"><span class="label">Medium</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
                                <div id="nbs">
                                    <h1>Nanobiosym</h1>
                                    <h2>Data Architecture & Data Science Manager Co-op</h2>
                                    <p>Location: Cambridge, Massachusetts, USA <br />
                                        June 2023  - December 2023
                                    </p>
                                    <h3>Tools: Airflow, AWS - Lambda, Glue, S3, RDS, SQS, ECR, EKS, Kubernetes, Docker, MLFlow, Sagemaker, HuggingFace, OpenAI, Quicksight</h3> 
                                    <ul>
                                        <li>
                                            <b>Scalable Data Architecture Implementation:</b> Spearheaded the implementation of a scalable data architecture and platforms to facilitate seamless integration and improve data management efficiency by 35%. Enabled in-depth analysis of extensive multi-channel data.
                                            <ul><li>
                                                <b>Challenge:</b> Addressing the complexities of multi-channel data and enhancing data management efficiency presented a significant challenge.
                                                </li><li>
                                                <b>Results:</b> Successfully implemented a scalable data architecture, improving data management efficiency by 35%. Facilitated in-depth analysis of multi-channel data, enhancing overall data insights.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Architectural Design for Enterprise Data:</b> Led architectural design initiatives to address complex enterprise data needs, resulting in a 20% increase in system efficiency and data accessibility.
                                            <ul><li>
                                                <b>Challenge:</b> Meeting the complex data requirements of the enterprise while ensuring system efficiency was a major challenge.
                                                </li><li>
                                                <b>Results:</b> Successfully designed and implemented architectural solutions, resulting in a 20% increase in system efficiency and improved data accessibility for enterprise users.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Generative AI Platform Development:</b> Delivered an AI/Generative AI platform with cloud-agnostic capabilities to maximize business performance in 3 months. Leveraged microservices, Kubernetes, and Docker.
                                            <ul><li>
                                                <b>Challenge:</b> Developing a cloud-agnostic AI/Generative AI platform within a tight timeframe posed a significant challenge.
                                                </li><li>
                                                <b>Results:</b> Successfully delivered the platform within three months, leveraging microservices, Kubernetes, and Docker. Enabled businesses to maximize performance through AI and generative AI capabilities.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Optimized Data Ingestion Workflows:</b> Optimized data ingestion workflows by combining real-time and batch processes with AWS technologies and big data tools. Resulted in a 40% enhancement in system efficiency and improved ETL/ELT processes.
                                            <ul><li>
                                                <b>Challenge:</b> Streamlining data ingestion workflows to enhance efficiency and improve ETL/ELT processes was a key challenge.
                                                </li><li>
                                                <b>Results:</b> Successfully optimized data ingestion workflows, achieving a 40% enhancement in system efficiency. Improved ETL/ELT processes, contributing to overall data processing efficiency.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Deep Learning Frameworks and Model Accuracy Improvement:</b> Delivered a 20% improvement in model accuracy by effectively employing deep learning frameworks (TensorFlow, PyTorch, scikit-learn) and open-source tools. Excelled in Python, Spark, Scala, and R assessments, driving data-driven solutions.
                                            <ul><li>
                                                <b>Challenge:</b> Enhancing model accuracy using diverse deep learning frameworks and tools presented a substantial challenge.
                                                </li><li>
                                                <b>Results:</b> Implemented deep learning frameworks effectively, resulting in a 20% improvement in model accuracy. Demonstrated proficiency in multiple programming languages, contributing to data-driven solutions.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Clinical Trials Data Visualization:</b> Drove the development of patient progress visualizations in clinical trials utilizing AWS Quicksight. Integrated metadata and data mapping to achieve 100% data security compliance, ensuring patient data privacy.
                                            <ul><li>
                                                <b>Challenge:</b> Creating a robust visualization system for clinical trial data while ensuring compliance with strict data security regulations was a primary challenge.
                                                </li><li>
                                                <b>Results:</b> Successfully implemented patient progress visualizations, enhancing the overall understanding of clinical trial data. Achieved 100% data security compliance, safeguarding patient privacy.
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                                
                                <div id="p360">
                                    <h1>Prescriber 360</h1>
                                    <h2>Lead Data Engineer</h2>
                                    <p>Location: Mumbai, India <br />
                                        September 2021  - April 2022
                                    </p>
                                    <h3>Tools: Azure - Azure Data Factory (ADF), Synapse, Data Lake (ADLS Gen2), Spark, Python, Databricks, Deltalake, SQL Server, Power BI</h3>
                                    <ul>
                                        <li>
                                            <b>End-to-End Data Solutions:</b> Engineered comprehensive end-to-end data solutions using a stack of Azure technologies. Leveraging Spark, Python, Databricks, Deltalake, Azure Data Factory (ADF), Synapse, and Power BI, I designed a modular data architecture that enabled seamless integration and processing of healthcare data.
                                            <ul><li>
                                                <b>Challenge:</b> Handling the diverse and complex healthcare data sources while ensuring data quality, compliance, and security was a significant challenge.
                                                </li><li>
                                                <b>Results:</b> Reduced data processing time by 40% and improved data accuracy by implementing a data validation automation framework. This resulted in a 25% reduction in data-related errors and a 30% increase in overall data processing efficiency.
                                            </ul>
                                        </li>

                                        <li>
                                            <b>Schema Drift:</b> Spearheaded scalable pipelines which catered to common data warehousing bottlenecks like Slowly Changing Dimensions (SCD), handling late arriving data, and facilitating data exports.
                                            <ul><li>
                                                <b>Challenge:</b> Managing evolving healthcare data with SCD Type 2 requirements while ensuring minimal impact on performance and data integrity posed a significant challenge.
                                                </li><li>
                                                <b>Results:</b> Implemented a robust SCD framework, reducing data discrepancies by 60%. Designed a late arriving data processing pipeline that accommodated up to 20% delayed data, ensuring data completeness. Automated data exports to various formats, enabling seamless sharing of insights with stakeholders.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Efficiency Improvement:</b> Significantly reduced data processing time by implementing batch data ingestion (ELT) from 100+ sources using Azure Data Factory (ADF) and Databricks, while adhering to best practices for data warehousing.
                                            <ul><li>
                                                <b>Challenge:</b> Coordinating complex data transformations, ensuring data quality, and optimizing ELT processes for parallelism was a multifaceted challenge.
                                                </li><li>
                                                <b>Results:</b> Achieved an average data ingestion rate of 2 terabytes per hour with parallelized processing. Reduced ELT job runtime by 50% through query optimization and distributed processing, resulting in near-real-time data availability for healthcare analytics.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Data Governance:</b> Implemented a robust data governance framework to ensure compliance, safeguard sensitive healthcare data, and enforce data retention policies in line with data warehousing best practices.
                                            <ul><li>
                                                <b>Challenge:</b> Balancing data accessibility with strict compliance requirements, including data retention and disposal policies, demanded a comprehensive approach.
                                                </li><li>
                                                <b>Results:</b> Achieved a 99% compliance rate with data retention policies, reducing storage costs by 15%. Implemented data masking techniques to protect sensitive patient data (PHI). Developed an automated data lifecycle management system, reducing manual intervention by 80%.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Scalable Data Pipelines:</b> Designed and developed scalable data pipelines using data warehousing techniques to accommodate growing healthcare data volumes.
                                            <ul><li>
                                                <b>Challenge:</b> Handling exponential data growth, maintaining data lineage, and ensuring scalability while minimizing disruptions to existing workflows were critical challenges.
                                                </li><li>
                                                <b>Results:</b> Scalable architecture handled a 200% increase in data volume within six months without impacting performance. Implemented data lineage tracking, ensuring data traceability for audit and compliance purposes. Reduced pipeline deployment time by 30% through automation.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Optimized Data Processing:</b> Continuously monitored and optimized data processing workflows on Azure using data warehousing concepts, enhancing performance, reducing costs, and ensuring data consistency.
                                            <ul><li>
                                                <b>Challenge:</b> Balancing performance optimization with cost control, managing data transformation dependencies, and implementing data quality checks across pipelines required meticulous planning.
                                                </li><li>
                                                <b>Results:</b> Reduced data processing costs by 20% through automated resource scaling and intelligent query caching. Implemented data quality checks, reducing data error rates by 75%. Ensured data consistency across various data marts and analytics tools.
                                            </ul>
                                        </li>
                                    </ul>
                                </div>

                                <hr />

                                <div id="cts">
                                    <h1>Cognizant</h1>
                                    <h2>Senior Data Engineer</h2>
                                    <p>Location: Kolkata, India <br />
                                        October 2019 - September 2021
                                    </p>
                                    <h3>Tools: AWS - Lambda, Glue, Kinesis Data Streams, Kinesis Firehose, S3, Athena, RDS, SQS, DynamoDB, DMS, SNS, SES</h3>
                                    <ul>
                                        <li>
                                            <b>Hybrid AWS-Based Client Broker Interface:</b> Designed and implemented a hybrid AWS-based client broker interface utilizing Amazon RDS and Snowflake, catering to the unique needs of the insurance industry.
                                            <ul><li>
                                                <b>Challenge:</b> Ensuring seamless integration with legacy systems, data consistency, and scalability were key challenges in this project.
                                                </li><li>
                                                <b>Results:</b> Achieved a remarkable 65% increase in customer satisfaction by providing real-time access to insurance data, enabling faster policy processing and claims management. Reduced data retrieval times by 40%, resulting in a more responsive and user-friendly interface.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Scalable Cloud Architecture:</b> Designed and implemented a scalable AWS cloud architecture for insurance data storage and processing, ensuring high availability and disaster recovery capabilities.
                                            <ul><li>
                                                <b>Challenge:</b> Balancing cost-efficiency with scalability while ensuring data security and compliance with industry regulations was a complex task.
                                                </li><li>
                                                <b>Results:</b> Established a resilient cloud infrastructure that achieved 99.9% uptime, providing continuous access to mission-critical insurance data. Reduced infrastructure costs by 15% through automated resource scaling and optimized cloud utilization.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Pioneering Real-time Streaming Pipelines:</b> Spearheaded the development of real-time data streaming pipelines leveraging AWS services like Amazon Kinesis, AWS Lambda, and AWS Glue, revolutionizing data processing efficiency.
                                            <ul><li>
                                                <b>Challenge:</b> Managing high data volumes in real-time, ensuring data accuracy, and minimizing processing delays were primary challenges.
                                                </li><li>
                                                <b>Results:</b> Achieved a groundbreaking 50% reduction in data processing time, enabling near-instantaneous access to critical insurance data. Implemented data quality checks and alerting mechanisms, reducing data errors by 80% and ensuring data accuracy.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Data Accuracy Enhancement:</b> Implemented robust data accuracy improvement measures, including data reconciliation, change data capture (CDC), and data transformation techniques.
                                            <ul><li>
                                                <b>Challenge:</b> Ensuring data accuracy in a dynamic insurance environment with frequent data changes and updates presented a significant challenge.
                                                </li><li>
                                                <b>Results:</b> Enhanced data accuracy by an impressive 90% through advanced data reconciliation algorithms and CDC mechanisms. Automated data transformation processes reduced manual intervention by 70%, ensuring accurate and up-to-date insurance data.
                                            </ul>
                                        </li>
                                    </ul>
                                </div>

                                <hr />

                                <div id="tcs">
                                    <h1>Tata Consultancy Services</h1>
                                    <h2>IT Analyst</h2>
                                    <p>Location: Kolkata, India <br />
                                        December 2015 - September 2019
                                    </p>
                                    <h3>Tools: Python, Node.js, Java, HDFS, Pig, Hive, Scoop, Informatica</h3>
                                    <ul>
                                        <li>
                                            <b>Large-Scale Data Migration:</b> Successfully orchestrated the migration of extensive data and codebases from legacy mainframe systems to modern solutions while minimizing data loss and ensuring a seamless transition.
                                            <ul><li>
                                                <b>Challenge:</b> Managing the migration of vast and complex datasets while minimizing downtime and ensuring data consistency was a critical challenge.
                                                </li><li>
                                                <b>Results:</b> Executed data migration projects with minimal disruption to business operations. Achieved a 99.5% data accuracy rate post-migration, ensuring that critical BFSI operations continued without interruption.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Data Integrity and Business Requirements:</b> Collaborated with business stakeholders to understand data requirements and designed ETL workflows to ensure data integrity and meet specific business needs.
                                            <ul><li>
                                                <b>Challenge:</b> Balancing data transformation requirements with business objectives and ensuring that data met compliance and regulatory standards was a multifaceted task.
                                                </li><li>
                                                <b>Results:</b> Implemented ETL workflows that maintained data consistency, accuracy, and compliance with BFSI regulations. Reduced data-related errors by 85% through data validation and quality checks.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>ETL Process Design:</b> Created and optimized ETL (Extract, Transform, Load) processes to facilitate data migration, transformation, and integration between mainframe and modern systems.
                                            <ul><li>
                                                <b>Challenge:</b> Ensuring efficient data transformation, handling data discrepancies, and optimizing ETL workflows for performance were key challenges.
                                                </li><li>
                                                <b>Results:</b> Designed and automated ETL processes that reduced data transformation time by 40% and enhanced data consistency. Implemented data lineage tracking to ensure traceability and auditability.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Technical Documentation:</b> Maintained comprehensive documentation of data migration processes, ETL workflows, and data lineage to facilitate knowledge sharing and project continuity.
                                            <ul><li>
                                                <b>Challenge:</b> Ensuring that knowledge was accessible to the team and future stakeholders was essential for long-term project success.
                                                </li><li>
                                                <b>Results:</b> Developed clear and accessible technical documentation, reducing onboarding time for new team members by 25%. Conducted knowledge-sharing sessions to promote cross-functional collaboration.
                                            </ul>
                                        </li>
                                        <li>
                                            <b>Collaborative Teamwork:</b> Collaborated with cross-functional teams, including business analysts, developers, and quality assurance teams, to ensure the successful execution of data migration projects.
                                            <ul><li>
                                                <b>Challenge:</b> Effective communication and coordination between technical and non-technical teams were crucial for project success.
                                                </li><li>
                                                <b>Results:</b> Fostered a collaborative work environment, resulting in the timely delivery of data migration projects that met or exceeded business expectations.
                                            </ul>
                                        </li>
                                    </ul>
                                </div>

                                <hr />

                                <div id="itc">
                                    <h1>ITC Infotech</h1>
                                    <h2>Associate IT Consultant</h2>
                                    <p>Location: Bengaluru, India <br />
                                        July 2013 - November 2015
                                    </p>
                                    <h3>Tools: Mainframe, IBM DB2, HP Service Now, Quality Center, Changeman</h3>
                                    <ul>
                                        <li>
                                            <b>Data Warehouse Developer</b> 
                                            <ul><li>
                                                    Requirement Gathering, Analysis, Development, Enhancement, Testing, Maintenance and Support of overall Data Warehouse Team.
                                                </li><li>
                                                    Proficient with Exploratory Data Analysis, ETL, Transformation, Clustering, Indexing and performance analysis.
                                                </li><li>
                                                    Resolved post production defects registered in HP Quality Center and Service Now.
                                                </li><li>
                                                    Preparation of unit test cases for different coding scenarios and conduct unit testing.
                                                </li><li>
                                                    Excellent knowledge of Incident, Problem, Change and Release Management.
                                                </li><li>
                                                    Support in the preparation of code review document.
                                                </li><li>
                                                    Conduct timely structured code reviews to ensure standards and systems inter-operability.
                                                </li><li>
                                                    Reporting Finnvera(Finland), Garantia Reports, Tapiola Reports, NIB(National Irish Bank) Index Updates
                                                </li><li>
                                                    Prepared client consumable presentations with actionable insights for data driven decision making.
                                                </li><li>
                                                <b>Results:</b> 
                                            </ul>
                                        </li>
                                        
                                    </ul>
                                </div>

                            </section>
                            	
                    </div>




                    <hr />

                    <!-- Footer -->
                        <footer id="footer">
                            <section class="split contact">
                                <section class="alt">
                                    <h3>Location</h3>
                                    <p>Boston, MA 02118</p>
                                </section>
                                <section>
                                    <h3>Phone</h3>
                                    <p><a href="tel:+17816006019">+1 (781) 600 6019</a></p>
                                </section>
                                <section>
                                    <h3>Email</h3>
                                    <p><a href="mailto: saha.deb@northeastern.edu">saha.deb@northeastern.edu</a></p>
                                </section>
                                <section>
                                    <h3>Social</h3>
                                    <ul class="icons alt">
                                        <li><a href="https://www.linkedin.com/in/debanjanaiml/" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
                                        <li><a href="https://github.com/debanjansaha-git" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
                                        <li><a href="http://www.youtube.com/@user-wf8dm8wu9b" class="icon brands alt fa-youtube"><span class="label">YouTube</span></a></li>
                                        <li><a href="https://medium.com/@dsdojo" class="icon brands alt fa-medium"><span class="label">Medium</span></a></li>
                                        <li><a href="https://wa.me/17816006019" class="icon brands alt fa-ehatsapp"><span class="label">Medium</span></a></li>
                                    </ul>
                                </section>
                            </section>
                        </footer>
    
                <!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Debanjan Saha</li><li>Design: Made with &hearts; by Debanjan and hosted by GitHub</li><li>All rights reserved 2024</li></ul>
					</div>
    
    
            </div>
    
            <!-- Scripts -->
                <script src="assets/js/jquery.min.js"></script>
                <script src="assets/js/jquery.scrollex.min.js"></script>
                <script src="assets/js/jquery.scrolly.min.js"></script>
                <script src="assets/js/browser.min.js"></script>
                <script src="assets/js/breakpoints.min.js"></script>
                <script src="assets/js/util.js"></script>
                <script src="assets/js/main.js"></script>
    
        </body>
    </html>
    